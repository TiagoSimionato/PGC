# PGC : Mobile testing with LLM

## Dependencies

### Virtual Environment

First setup de python virtual environment

```bash
python -m venv .venv
```

### Packages

The necessary packages and the corresponding versions are declared under `requirements.txt` file. Note that langchain requires C++ build tools in order to be installed.

```bash
pip install -r requirements.txt
```

In case `langserve` is not installed sucessfully try the following:

```bash
pip install "langserve[all]"
```

### LLM Models

[Download the Mini Orca LLM Model](https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf)

[Download the GPT4ALL Falcon LLM Model](https://gpt4all.io/models/gguf/gpt4all-falcon-q4_0.gguf)

[Download the WizardLM (larger) LLM Model](https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf)

By default, models should be stored inside a `models` folder, but the desired path can be adjusted in `configs.py`

### Appium

The project uses appium for android emulator automation. It aims to extract ui data to feed the LLM prompt so it can generate test to run with the Appium tool itself.

```bash
npm i -g appium

appium driver install uiautomator2
```

version: v2.5.1

uiautomator2@3.0.1

It is required to set `ANDROID_HOME` and `JAVA_HOME` environment variables

## Running

Activating venv

```bash
.venv\Scripts\activate.bat
```

### Local Models

It is possible to run models locally from GPT4All. Start by initializing the server

```bash
python src/llmServer.py
```

it will load the model and serve it as a REST API under the port `8000` and path `/query`

By default it searches for the Wizard language model at the models directory, but it can be specified when initializing the server. Use options `1`, `2` or `3` according to the size of the model desired, being `3` the largest.

```bash
python src/llmServer.py 2
```

## Client

```bash
python src/client.py
```

The client first connects with Appium to extract information from the app interface. Then it will generate the prompt with the collected info and send it to the selected LLM model. The answer is saved inside a .txt file containing the prompt folled by the answer generated by the model.

### Flags

#### appiumPort

```bash
python src/client.py --appiumPort 1234
```

it not provided it is set with appium default 4723

#### model

```bash
python src/client.py --model 2
```

Indicates if the client should connect with gemini or a local server. Options can be chosen with `1` or `2`, respectively. Gemini is used by default as it's generally a better model.

#### outputPath

```bash
python src/client.py --outputPath you_path_here
```

Used to change default path where generated answers are stored

#### localModelName

```bash
python src/client.py --localModelName model_name_here
```

Used to name the output file when using local models
